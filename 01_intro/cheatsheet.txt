docker run -it -rm apachae/kafka:latest bash

# generate a random uuid for cluster id
docker run --rm apache/kafka:latest /opt/kafka/bin/kafka-storage.sh random-uuid

# when the image/container is running
docker exec -it kafka-broker bash

# create a topic
docker exec -it kafka-broker /opt/kafka/bin/kafka-topics.sh --create --topic my-first-topic --partitions 1 --replication-factor 1 --bootstrap-server kafka:9092
docker exec -it kafka1 /opt/kafka/bin/kafka-topics.sh --create --topic photo-editing-queue --partitions 2 --replication-factor 3 --bootstrap-server kafka:29092

# generate meta.properties for kafka
docker run --rm -v $(pwd)/kafka-data:/tmp/kraft-data apache/kafka:latest /opt/kafka/bin/kafka-storage.sh format --cluster-id <generated_cluster_id> --config /opt/kafka/config/server.properties --standalone
# Sample:  docker run --rm -v $(pwd)/kafka-data:/tmp/kraft-data apache/kafka:latest /opt/kafka/bin/kafka-storage.sh format --cluster-id Eq2RE98JRhiAGPiQ0AJH6Q --config /opt/kafka/config/server.properties --standalone

# up the services but excluded old services thats been stopped by us
docker compose up -d --remove-orphan

# Create consumer
docker exec -it kafka1 /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:29092 --topic town-annoucement --from-beginning
docker exec -it kafka1 /opt/kafka/bin/kafka-console-consumer.sh \
--topic order-events \
--bootstrap-server kafka1:29092,kafka2:29092,kafka3:29092 \
--formatter org.apache.kafka.tools.consumer.DefaultMessageFormatter \
--property print.timestamp=true \
--property print.key=true \
--property print.offset=true \
--property print.partition=true \
--producer-property partitioner.class=org.apache.kafka.clients.producer.RoundRobinPartitioner

# Kafka describe
docker exec -it kafka1 /opt/kafka/bin/kafka-topics.sh --describe --topic acks-demo-topic --bootstrap-server kafka1:29092


# Create consumer with group processing
docker exec -it kafka1 /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:29092 --topic town-annoucement --grop processors

# Create messages to topic
docker exec -it kafka1 /opt/kafka/bin/kafka-console-producer.sh --topic photo-processing-queue --bootstrap-server kafka1:29092

# Create messages to topic with Queue key
docker exec -it kafka1 /opt/kafka/bin/kafka-console-producer.sh --topic photo-processing-queue --bootstrap-server kafka1:29092 --property "parse.key=true" --property "key.separator=:"

# Kafka Segment IO
github.com/segmentio/kafka-go

# Open a container with kafka connect configuration
docker run --rm -it \
--name connect-worker \
--network kafka-4-cluster-multinode_kafka-net \
-v ./connect-standalone.properties:/tmp/connect-standalone.properties \
-v ./file-source.properties:/tmp/file-source.properties \
-v ./input.txt:/tmp/input.txt \
apache/kafka:4.0.0 /opt/kafka/bin/connect-standalone.sh \
/tmp/connect-standalone.properties \
/tmp/file-source.properties

# Check Distributed Kafka file status
curl http://localhost:8083/connectors/string-file-source/status

# Delete Distributed Kafka file status
curl -X DELETE http://localhost:8083/connectors/distributed-file-source

# POST Distributed Kafka file status
curl -X POST -H "Content-Type: application/json" --data '@file-source-config.json' http://localhost:8083/connectors

# reassign topics volume to different brokers
docker exec kafka1 /opt/kafka/bin/kafka-reassign-partitions.sh \
--bootstrap-server kafka1:29092 \
--topics-to-move-json-file /tmp/topics-to-move.json \
--broker-list "1,2,3,4" \
--generate

# reassign brokers data to other brokers
docker exec kafka1 /opt/kafka/bin/kafka-reassign-partitions.sh \
--bootstrap-server kafka1:29092 \
--reassignment-json-file /tmp/reassignment-plan.json \
--execute